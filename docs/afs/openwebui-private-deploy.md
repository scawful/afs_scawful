# Open WebUI Private Deployment (halext-nj)

Goal: run Open WebUI on `chat.halext.org`, authenticated via halext.org SSO so
only `scawful` and `magicalgirl` can access it. The instance connects to the
Windows GPU Ollama tunnel and reuses the same cloud provider secrets.

## Current Deployment (halext-nj)

- Open WebUI runs via `docker/docker-compose.halext-nj.yml`.
- Bound to `127.0.0.1:3002` (port 3000 is taken by Gitea; 3001 is in use).
- Nginx proxies `/` to Open WebUI and `/ajax/` to the legacy PHP chat.
- Auth uses `auth.halext.org` `/verify` via `auth_request`.
- Nginx allowlist restricts access to `scawful` and `magicalgirl`.
- Ollama tunnel runs as a systemd user service (`afs-ollama-tunnel.service`).
- LiteLLM starts automatically when Anthropic/Gemini keys are present.

## Deploy/Update Steps

1. Sync `lab/afs` on `halext-nj`.
2. Create secrets on the server (do not commit):
   - `~/.config/afs/secrets.env` (OpenAI/OpenRouter/Anthropic or `CLAUDE_API_KEY`/Gemini keys)
   - `~/.config/afs/openwebui.secrets.env` (generated by `chat-service.sh`)
   - `~/.config/afs/litellm.env` + `~/.config/afs/litellm.yaml` (generated when needed)
3. Ensure SSH access from `halext-nj` to `medical-mechanica`:
   - Add `~/.ssh/config` host entry for `medical-mechanica`
   - Add the `halext-nj` public key to Windows `administrators_authorized_keys`
4. Enable the tunnel:
   ```bash
   systemctl --user enable --now afs-ollama-tunnel.service
   ```
5. Start Open WebUI:
   ```bash
   AFS_CHAT_COMPOSE_FILE=docker-compose.halext-nj.yml \
   AFS_CHAT_URL=https://chat.halext.org \
   ./scripts/chat-service.sh start simple
   ```
6. Update Nginx (`/etc/nginx/sites/chat.halext.org.conf`) and reload.

## Nginx Notes

- Proxy target: `http://127.0.0.1:3002`
- Auth check: `http://127.0.0.1:8001/verify`
- Trusted headers: `X-User-ID`, `X-Username`, `X-User-Email`
- `/ajax/` serves the legacy PHP chat via `php8.3-fpm`
- Restrict to `scawful` and `magicalgirl` in the `/` location

## Model Sources

Keep Windows GPU models on `medical-mechanica`. Local models on `halext-nj`
should stay minimal. The Open WebUI tagging logic in `chat-service.sh` expects
Windows on index 0 and local on index 1.
